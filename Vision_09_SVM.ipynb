{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vision_09_SVM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"code","metadata":{"id":"qhzkZfVXE7G-"},"source":["# Dataset Caltech 101\n","# Source: http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html\n","# Use 3 classes: elephant, rooster, rhino"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvWHqnpAqfO-"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","from os import listdir\n","from os.path import join\n","from skimage.io import imread\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtLAE8EDPDTH","executionInfo":{"status":"ok","timestamp":1604192755896,"user_tz":360,"elapsed":39159,"user":{"displayName":"Sebastian Valderrábano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghbr0GuiiP03Or_UAajc0KapOVP2KnJyEmai2Micw=s64","userId":"13462043322002867981"}},"outputId":"2324047b-00d0-448b-c746-dd5d7b1daf62","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vSu2Nh0aqfPH","executionInfo":{"status":"ok","timestamp":1604192837627,"user_tz":360,"elapsed":64689,"user":{"displayName":"Sebastian Valderrábano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghbr0GuiiP03Or_UAajc0KapOVP2KnJyEmai2Micw=s64","userId":"13462043322002867981"}},"outputId":"ae07f837-f003-4762-8f2c-95c889aaeeda","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Find the name of each class\n","base_path = \"/content/drive/My Drive/Data_FromCaltech\"\n","class_names = listdir(base_path)\n","print(\"Num of classes:\", len(class_names))\n","\n","# Load all images from each class and label them. Use original size.\n","I = []\n","Y = np.empty(shape=(0))\n","for cl in range(len(class_names)):\n","    files = [join(base_path, class_names[cl], f) for f in listdir(join(base_path, class_names[cl]))]\n","    for file_path in files:\n","        I.append(imread(file_path))\n","    Y = np.concatenate((Y, cl * np.ones((len(files))) ))\n","    \n","print(len(I))\n","print(Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num of classes: 5\n","304\n","(304,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tMYuQwnPzl5H","executionInfo":{"status":"error","timestamp":1604193017313,"user_tz":360,"elapsed":889,"user":{"displayName":"Sebastian Valderrábano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghbr0GuiiP03Or_UAajc0KapOVP2KnJyEmai2Micw=s64","userId":"13462043322002867981"}},"outputId":"d95ab9e4-8d4a-4022-ba9a-0b6152cd713d","colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["pip install opencv-python==3.4.2.16\n","pip install opencv-contrib-python==3.4.2.16\n","#Este codigo esta para evitar que cv2 nos marque error"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-03c5e5df3b75>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install opencv-python==3.4.2.16\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"5hLZYnZdqfPO","executionInfo":{"status":"error","timestamp":1604194176178,"user_tz":360,"elapsed":377,"user":{"displayName":"Sebastian Valderrábano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghbr0GuiiP03Or_UAajc0KapOVP2KnJyEmai2Micw=s64","userId":"13462043322002867981"}},"outputId":"5cb8c651-4b2b-4228-c16c-5693f2947fe9","colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["# Compute SIFT for all images\n","sift_model = cv2.xfeatures2d.SIFT_create()\n","SIFTs = []\n","for img in I:\n","    _, s = sift_model.detectAndCompute(img, None)\n","    SIFTs.append(s)\n","\n","print(SIFTs[0].shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-2698e488b312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute SIFT for all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msift_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mSIFTs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv_contrib/modules/xfeatures2d/src/sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n"]}]},{"cell_type":"code","metadata":{"id":"_DdKZPq_qfPT"},"source":["# Split data into training and validation sets\n","SIFTs_train, SIFTs_val, y_train, y_val = train_test_split(SIFTs, Y, test_size=0.2)\n","\n","print(len(SIFTs_train))\n","print(len(SIFTs_val))\n","print(len(y_train))\n","print(len(y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LggQA6yOqfPZ"},"source":["# Count the total number of sift descriptors in the corpus.\n","num_sifts = 0\n","for s in SIFTs_train:\n","    num_sifts = num_sifts + len(s)\n","print(f\"There are {num_sifts} SIFT descriptors in the training set.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUBjvXc3qfPf"},"source":["# Prepare training set for clustering\n","trainingSIFTS = np.empty(shape=(0, 128))\n","for s in SIFTs_train:\n","    trainingSIFTS = np.concatenate((trainingSIFTS, s))\n","\n","print(trainingSIFTS.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrB3gqs2qfPm"},"source":["# Find visual vocabulary: Train a k-means clustering\n","n_words = 200\n","kmeans = KMeans(n_clusters=n_words).fit(trainingSIFTS) # This might take around 5 minutes for 70k SIFTs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uhS2h_pqfPq"},"source":["# Process all SIFTs from training set\n","X_train = np.zeros((len(SIFTs_train), n_words))\n","for ind, s in enumerate(SIFTs_train):\n","    words = kmeans.predict(s.astype(float))   # assign labels\n","    bow, _ = np.histogram(words, range(n_words + 1))\n","    X_train[ind] = bow / bow.sum()\n","\n","print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jjj7mtWJqfPv"},"source":["# Normalize data with a standar scaler\n","scaler = MinMaxScaler().fit(X_train)\n","X_train = scaler.transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGEPO0-cqfP0"},"source":["# Process all SIFTs from validation set\n","X_val = np.zeros((len(SIFTs_val), n_words))\n","for ind, s in enumerate(SIFTs_val):\n","    words = kmeans.predict(s.astype(float))   # assign labels\n","    bow, _ = np.histogram(words, range(n_words + 1))\n","    X_val[ind] = bow / bow.sum()\n","\n","print(X_val.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDOuv7QkqfP4"},"source":["# Normalize data with a standar scaler\n","X_val = scaler.transform(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VApBtQNqfQC"},"source":["# Classify with NaiveBayes\n","clf_nb = MultinomialNB()\n","clf_nb.fit(X_train, y_train)\n","print(clf_nb.classes_)\n","print(clf_nb.class_count_)\n","\n","# Print\n","print(f\"Training mean accuracy: {clf_nb.score(X_train, y_train):6.4f}\")\n","print(f\"Test mean accuracy: {clf_nb.score(X_val, y_val):6.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFwCLLk4qfQH"},"source":["# Classify with SVM\n","clf_svm = SVC(kernel='linear')\n","clf_svm.fit(X_train, y_train)\n","print(clf_svm.classes_)\n","\n","# Print\n","print(f\"Training mean accuracy: {clf_svm.score(X_train, y_train):6.4f}\")\n","print(f\"Test mean accuracy: {clf_svm.score(X_val, y_val):6.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aFmpIX0UqfQL"},"source":["# Mini proyecto Bow and SVM"]},{"cell_type":"code","metadata":{"id":"E1Csc9FkqfQM"},"source":["# Usando los parámetros default de Naive Bayes y de SVC (excepto el kernel. usen lineal), encuentren el tamaño de vocabulario\n","# que proporcione el mejor promedio de exactitud de clasificación (en entrenamiento y test, traten de evitar sobre entrenamiento).\n","# Prueba los siguientes valores: 50, 100, 250, 500, 1000, 2000 palabras visuales.\n","# Nota, para 2000 palabras, kmeans puede tomar al rededor de 30 min, mientras que para 50 palabras tomará únicamente unos\n","# 2 minutos. Puedes usar el parámetro verbose para controlar si quieres información sobre lo que kmeans está haciendo.\n","\n","# == Q1 ==\n","# Reporta una tabla con los scores de clasificación para entrenamiento y validación para ambos modelos y los 6 tamaños de\n","# diccionario visual.\n","\n","val_palabras = [50, 100, 250, 500, 1000, 2000]\n","lista_val = []\n","for val in val_palabras:\n","  SIFTs_train, SIFTs_val, y_train, y_val = train_test_split(SIFTs, Y, test_size=0.2)\n","\n","  num_sifts = 0\n","  for s in SIFTs_train:\n","    num_sifts = num_sifts + len(s)\n","\n","  trainingSIFTS = np.empty(shape=(0, 128))\n","  for s in SIFTs_train:\n","    trainingSIFTS = np.concatenate((trainingSIFTS, s))\n","\n","  kmeans = KMeans(n_clusters=val).fit(trainingSIFTS) \n","\n","  X_train = np.zeros((len(SIFTs_train), val))\n","  for ind, s in enumerate(SIFTs_train):\n","    words = kmeans.predict(s.astype(float))   # assign labels\n","    bow, _ = np.histogram(words, range(val + 1))\n","    X_train[ind] = bow / bow.sum()\n","\n","  scaler = MinMaxScaler().fit(X_train)\n","  X_train = scaler.transform(X_train)\n","\n","  X_val = np.zeros((len(SIFTs_val), val))\n","  for ind, s in enumerate(SIFTs_val):\n","    words = kmeans.predict(s.astype(float))   # assign labels\n","    bow, _ = np.histogram(words, range(val + 1))\n","    X_val[ind] = bow / bow.sum()\n","\n","  X_val = scaler.transform(X_val)\n","\n","  # Classify with NaiveBayes\n","  clf_nb = MultinomialNB()\n","  clf_nb.fit(X_train, y_train)\n","  #print(clf_nb.classes_)\n","  #print(clf_nb.class_count_)\n","\n","  # Print\n","  #print(f\"Training mean accuracy BAYES: {clf_nb.score(X_train, y_train):6.4f}\", \" CON VAL \", val)\n","  #print(f\"Test mean accuracy BAYES: {clf_nb.score(X_train, y_train):6.4f}\", \" CON VAL \", val)\n","\n","  # Classify with SVM\n","  clf_svm = SVC(kernel='linear')\n","  clf_svm.fit(X_train, y_train)\n","  #print(clf_svm.classes_)\n","\n","  # Print\n","  #print(f\"Training mean accuracy SVM: {clf_svm.score(X_train, y_train):6.4f}\", \" CON VAL \", val)\n","  #print(f\"Test mean accuracy SVM: {clf_svm.score(X_val, y_val):6.4f}\", \" CON VAL \", val)\n","  lista_val.append([val, clf_nb.score(X_train, y_train), clf_nb.score(X_train, y_train), clf_svm.score(X_train, y_train), clf_svm.score(X_val, y_val)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuyQJ6pu0aes"},"source":["#Impresión de valores para pregunta 1\n","import pandas as pd\n","dataf = pd.DataFrame(lista_val, columns=[\"Número de palabras\", \"Training mean accuracy BAYES\", \"Test mean accuracy BAYES\", \"Training mean accuracy SVM\", \"Test mean accuracy SVM\"])\n","dataf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaeGHzicqfQR"},"source":["# Ahora congelen el tamaño del vocabulario, usando el mejor obtenido anteriormente.\n","# Con Naive Bayes no hay mucho más que hacer, pero podemos explorar los hiperparámetros de SVM.\n","# Varien el tipo de kernel para SVC, probando linear, rbf y polinimial. Y varien los parámetros C y gamma.\n","# Pueden evaluar: 0.01, 0.05, 0.1, 0.5, 1, 5 para ambos parámetros\n","# Encuentren la combinación con el mejor desempeño de clasificación (en entrenamiento y test, eviten sobre entrenamiento).\n","# Revisen la documentación de GridSearch en sklearn, puede ayudarles a eficientar esta exploración.\n","\n","val = 100\n","\n","SIFTs_train, SIFTs_val, y_train, y_val = train_test_split(SIFTs, Y, test_size=0.2)\n","\n","num_sifts = 0\n","for s in SIFTs_train:\n","  num_sifts = num_sifts + len(s)\n","\n","trainingSIFTS = np.empty(shape=(0, 128))\n","for s in SIFTs_train:\n","  trainingSIFTS = np.concatenate((trainingSIFTS, s))\n","\n","kmeans = KMeans(n_clusters=val).fit(trainingSIFTS) \n","\n","X_train = np.zeros((len(SIFTs_train), val))\n","for ind, s in enumerate(SIFTs_train):\n","  words = kmeans.predict(s.astype(float))   # assign labels\n","  bow, _ = np.histogram(words, range(val + 1))\n","  X_train[ind] = bow / bow.sum()\n","\n","scaler = MinMaxScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","\n","X_val = np.zeros((len(SIFTs_val), val))\n","for ind, s in enumerate(SIFTs_val):\n","  words = kmeans.predict(s.astype(float))   # assign labels\n","  bow, _ = np.histogram(words, range(val + 1))\n","  X_val[ind] = bow / bow.sum()\n","\n","X_val = scaler.transform(X_val)\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.01, 0.05, 0.1, 0.5, 1, 5], 'gamma':[0.01, 0.05, 0.1, 0.5, 1, 5]}\n","\n","clf_svm = SVC()\n","clf_GS = GridSearchCV(clf_svm, parameters)\n","clf_GS.fit(X_train, y_train)\n","\n","\n","#print(f\"Training mean accuracy SVM: {clf_GS.score(X_train, y_train):6.4f}\", \" CON VAL \", val)\n","#print(f\"Test mean accuracy SVM: {clf_GS.score(X_val, y_val):6.4f}\", \" CON VAL \", val)\n","#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycobyGM0ktlc"},"source":["# == Q2 ==\n","# Para cada tipo de kernel, reporten una tabla con las combinaciones de C y gamma evaluadas y los desempeños (train, val)\n","# obtenidos.\n","\n","res = clf_GS.cv_results_\n","df = pd.DataFrame(data=res)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0ALptkxtLGq"},"source":["#Pregunta 2 separada por kernel reflejando los valores de cada iteración:\n","\n","kernel_S=['linear', 'rbf', 'poly']\n","C_S=[0.01, 0.05, 0.1, 0.5, 1, 5]\n","gamma_S=[0.01, 0.05, 0.1, 0.5, 1, 5]\n","dic = {}\n","i = 1\n","for ker in kernel_S:\n","  for ce in C_S:\n","    for gam in gamma_S:\n","\n","      val = 100\n","\n","      SIFTs_train, SIFTs_val, y_train, y_val = train_test_split(SIFTs, Y, test_size=0.2)\n","\n","      num_sifts = 0\n","      for s in SIFTs_train:\n","        num_sifts = num_sifts + len(s)\n","\n","      trainingSIFTS = np.empty(shape=(0, 128))\n","      for s in SIFTs_train:\n","        trainingSIFTS = np.concatenate((trainingSIFTS, s))\n","\n","      kmeans = KMeans(n_clusters=val).fit(trainingSIFTS) \n","\n","      X_train = np.zeros((len(SIFTs_train), val))\n","      for ind, s in enumerate(SIFTs_train):\n","        words = kmeans.predict(s.astype(float))   # assign labels\n","        bow, _ = np.histogram(words, range(val + 1))\n","        X_train[ind] = bow / bow.sum()\n","\n","      scaler = MinMaxScaler().fit(X_train)\n","      X_train = scaler.transform(X_train)\n","\n","      X_val = np.zeros((len(SIFTs_val), val))\n","      for ind, s in enumerate(SIFTs_val):\n","        words = kmeans.predict(s.astype(float))   # assign labels\n","        bow, _ = np.histogram(words, range(val + 1))\n","        X_val[ind] = bow / bow.sum()\n","\n","      X_val = scaler.transform(X_val)\n","\n","      clf_svm = SVC(kernel=ker,gamma=gam, C=ce)\n","      clf_svm.fit(X_train, y_train)\n","      print(clf_svm.classes_)\n","\n","      # Print\n","      print(f\"Training mean accuracy SVM: {clf_svm.score(X_train, y_train):6.4f}\", \" CON Gamma \",gam, \", C \",ce,\", kernel \", ker)\n","      dic[i] = [{clf_svm.score(X_train, y_train):6.4f}, gam, ce, ker]\n","      i += 1\n","      print(f\"Test mean accuracy SVM: {clf_svm.score(X_val, y_val):6.4f}\", \" CON Gamma \",gam, \", C \",ce,\", kernel \", ker)\n","      dic[i] = = [{clf_svm.score(X_val, y_val):6.4f}, gam, ce, ker]\n","      i += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kF4vrFrqqfQV"},"source":["# == Q3 ==\n","# De entre todos los modelos evaluados, elijan el que consideren mejor. Reporten su justificación para considerarlo el mejor.\n","\n","# De todos los modelos evaluados, que incluyeron kernel lineal, RBF y polinomial consideramos que el que presentó los\n","# resultados más satisfactorios fue el polinomial. Este con un valor C = 0.01 y gamma = 0.5. Esto nos dió un score que imprimimos\n","# que imprimimos en esta celda. Este modelo es mejor dado a que se tiene una alta media en ambos casos, \n","# demostrando que no se tiene caso de overfitting ni underfitting. Con estos parámetros se alcanzó una media de prueba más \n","# alta que la mayoría de los otros modelos, en los cuales el score fue menor. Otras pruebas demostraron\n","# un valor más alto en media de entrenamiento, pero ninguno tuvo una media de prueba igual o más alto que este. Y revisando\n","# para RBF y polinomial, los peores casos eran mucho menores que los resultados vistos. El score toma las medias de \n","# entrenamiento y prueba para sacar el mejor modelo, por lo que buscamos el que tenga mayor puntaje en esto. Así aseguramos\n","# la mayor precisión.\n","\n","print(\"La mejor score es:\")\n","print(clf_GS.best_score_)\n","\n","print(\"Los parámetros que alcanzaron esta score fueron: \")\n","print(clf_GS.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRnFEGVeqfQb"},"source":["# == Q4 ==\n","# Usando el mejor modelo de entre todos los probados, grafiquen y reporten la matriz de confusión.\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","mod_svm = SVC(kernel='poly',C=0.01, gamma=0.5)\n","modelo = mod_svm.fit(X_train, y_train)\n","\n","#La matriz de confusión no se encuentra normalizada, por lo cual los valores no van de 0 a 1\n","disp = plot_confusion_matrix(modelo, X_val, y_val, cmap=plt.cm.Blues)\n","disp.ax_.set_title(\"Matriz de confusión sin normalizar\")\n","print(\"Matriz de confusión sin normalizar:\")\n","print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrua45hyqfQi"},"source":["# == Q5 ==\n","# Con base en el mejor modelo (seleccionado)\n","# ¿Qué clase es la más fácil de clasificar? ¿Por qué creen que sea así?\n","\n","# Rooster o gallo es la más fácil de clasificar. Esto porque se necesitan menos palabras para asociar una imagen a \n","# este concepto. Al ser menos compleja de escribir, también se eliminan varias búsquedas que involucran verificar errores \n","# del usuario. Por lo tanto, el clasificador tomaría menos tiempo para relacionar las imágenes. Además que este animal tiene \n","# una forma reconocible para el algoritmo SIFT. El kernel RBF, al tomar el radio alrededor de un punto dado, puede rápidamente \n","# que los valores son menores que en los otros casos. Por lo que descarta opciones que no son relevantes a la búsqueda con mayor\n","# eficiencia.\n","\n","# ¿Cuáles clases son las que más se confunden entre sí? ¿Por qué creen que sea así?\n","\n","# Rhino y elephant son más confundidas entre sí. Esto dado que ambos tienen características similares, que al momento de \n","# ejecutar el algoritmo SIFT pueden ser difíciles de discernir. En cuanto forma, estas dos clases son más similares entre ellas\n","# comparando con rooster, por lo cual el radio alrededor de los puntos clave es parecido en ambos casos. \n","# Por lo que es necesario dar más tiempo para notar los detalles en cada imagen y categorizar correctamente cada una de estas. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JD_7OqiYqfQm"},"source":["# Bonus: Extender la exploración de la pregunta 2, para evaluar:\n","# 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50\n","# ¿Se logró alguna mejora?\n","val = 100\n","\n","SIFTs_train, SIFTs_val, y_train, y_val = train_test_split(SIFTs, Y, test_size=0.2)\n","\n","num_sifts = 0\n","for s in SIFTs_train:\n","  num_sifts = num_sifts + len(s)\n","\n","trainingSIFTS = np.empty(shape=(0, 128))\n","for s in SIFTs_train:\n","  trainingSIFTS = np.concatenate((trainingSIFTS, s))\n","\n","kmeans = KMeans(n_clusters=val).fit(trainingSIFTS) \n","\n","X_train = np.zeros((len(SIFTs_train), val))\n","for ind, s in enumerate(SIFTs_train):\n","  words = kmeans.predict(s.astype(float))   # assign labels\n","  bow, _ = np.histogram(words, range(val + 1))\n","  X_train[ind] = bow / bow.sum()\n","\n","scaler = MinMaxScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","\n","X_val = np.zeros((len(SIFTs_val), val))\n","for ind, s in enumerate(SIFTs_val):\n","  words = kmeans.predict(s.astype(float))   # assign labels\n","  bow, _ = np.histogram(words, range(val + 1))\n","  X_val[ind] = bow / bow.sum()\n","\n","X_val = scaler.transform(X_val)\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","parameters_b = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50], 'gamma':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50]}\n","\n","clf_svm_b = SVC()\n","clf_GS_b = GridSearchCV(clf_svm_b, parameters_b)\n","clf_GS_b.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Suxu9-t27Hek"},"source":["print(\"La mejor score con los valores adicionales es:\")\n","print(clf_GS_b.best_score_)\n","\n","print(\"Los parámetros que alcanzaron esta score fueron: \")\n","print(clf_GS_b.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFdBoalK7NG9"},"source":["# ¿Se logró alguna mejora?\n","# Si, pero varía dependiendo las iteraciones.\n","# En un primer intento la clasificación se realizó con la misma eficiencia, a pesar de agregar nuevos parámetros a revisar. \n","# Y con estos nuevos valores se alcanzó una score más alta que la anterior, aunque esta diferencia no sea tan grande como \n","# se quisiera. No obstante, con esto podemos identificar un kernel más eficiente, en este caso RBF, \n","# usando un valor C = 10 que no se tenía en la prueba anterior.  \n","# Sin embargo, al reiniciar el kernel y correr el proceso una segunda vez, se obtuvo un resultado distinto. El score fue más\n","# alto, alcanzando un 0.62, mientras que la primera tuvo un valor aproximado de 0.59 (cerca del 0.6 pero no lo alcanzaba).\n","# El caso en esta iteración, es que tanto los parámetros C como gamma se encontraban dentro de las posibilidades que \n","# se establecieron en la pregunta 2. Así que esto puede ser debido a eficiencia al realizar la prueba o recursos disponibles\n","# del ordenador.\n","\n","# Por lo tanto, en ambas pruebas encontramos una mejor score utilizando un rango mayor de parámetros. A pesar de que los valores\n","# ideales se encontraban en ambas listas definidas, en algunos casos las mejoras provienen de un uso de C o gamma presente\n","# únicamente en esta extensión de la exploración."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewru70YGqfQp"},"source":["# Entrega. PDF, con nombres. Se vale que sea la impresión de las salidas del notebook, pero excluyan la parte de muestra.\n","# Incluyan sólo lo que corresponde a responder las preguntas.\n","# Deadline: Lunes 5 de octubre, 18:00 hr."],"execution_count":null,"outputs":[]}]}